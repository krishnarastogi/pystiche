{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Model optimization\n\nThis example showcases how an NST based on model optimization can be performed in\n``pystiche``. It closely follows the\n`official PyTorch example <https://github.com/pytorch/examples/tree/master/fast_neural_style>`_\nwhich in turn is based on :cite:`JAL2016`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start this example by importing everything we need and setting the device we will\nbe working on.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import contextlib\nimport os\nimport time\nfrom collections import OrderedDict\nfrom os import path\n\nimport torch\nfrom torch import hub, nn\nfrom torch.nn.functional import interpolate\n\nimport pystiche\nfrom pystiche import demo, enc, loss, ops, optim\nfrom pystiche.image import show_image\nfrom pystiche.misc import get_device\n\nprint(f\"I'm working with pystiche=={pystiche.__version__}\")\n\ndevice = get_device()\nprint(f\"I'm working with {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer\n\nIn contrast to image optimization, for model optimization we need to define a\ntransformer that, after it is trained, performs the stylization. In general different\narchitectures are possible (:cite:`JAL2016,ULVL2016`). For this example we use an\nencoder-decoder architecture.\n\nBefore we define the transformer, we create some helper modules to reduce the clutter.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the decoder we need to upsample the image. While it is possible to achieve this\nwith a :class:`~torch.nn.ConvTranspose2d`, it was found that traditional upsampling\nfollowed by a standard convolution\n`produces fewer artifacts <https://distill.pub/2016/deconv-checkerboard/>`_. Thus,\nwe create an module that wraps :func:`~torch.nn.functional.interpolate`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Interpolate(nn.Module):\n    def __init__(self, scale_factor=1.0, mode=\"nearest\"):\n        super().__init__()\n        self.scale_factor = scale_factor\n        self.mode = mode\n\n    def forward(self, input):\n        return interpolate(input, scale_factor=self.scale_factor, mode=self.mode,)\n\n    def extra_repr(self):\n        extras = []\n        if self.scale_factor:\n            extras.append(f\"scale_factor={self.scale_factor}\")\n        if self.mode != \"nearest\":\n            extras.append(f\"mode={self.mode}\")\n        return \", \".join(extras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the transformer architecture we will be using, we need to define a convolution\nmodule with some additional capabilities. In particular, it needs to be able to\n- optionally upsample the input,\n- pad the input in order for the convolution to be size-preserving,\n- optionally normalize the output, and\n- optionally pass the output through an activation function.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Instead of :class:`~torch.nn.BatchNorm2d` we use :class:`~torch.nn.InstanceNorm2d`\n  to normalize the output since it gives better results for NST :cite:`UVL2016`.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Conv(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        upsample=False,\n        norm=True,\n        activation=True,\n    ):\n        super().__init__()\n        self.upsample = Interpolate(scale_factor=stride) if upsample else None\n        self.pad = nn.ReflectionPad2d(kernel_size // 2)\n        self.conv = nn.Conv2d(\n            in_channels, out_channels, kernel_size, stride=1 if upsample else stride\n        )\n        self.norm = nn.InstanceNorm2d(out_channels, affine=True) if norm else None\n        self.activation = nn.ReLU() if activation else None\n\n    def forward(self, input):\n        if self.upsample:\n            input = self.upsample(input)\n\n        output = self.conv(self.pad(input))\n\n        if self.norm:\n            output = self.norm(output)\n        if self.activation:\n            output = self.activation(output)\n\n        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is common practice to append a few residual blocks after the initial convolutions\nto the encoder to enable it to learn more descriptive features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.conv1 = Conv(channels, channels, kernel_size=3)\n        self.conv2 = Conv(channels, channels, kernel_size=3, activation=False)\n\n    def forward(self, input):\n        output = self.conv2(self.conv1(input))\n        return output + input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be useful for the training to transform the input into another value range,\nfor example from $\\closedinterval{0}{1}$ to $\\closedinterval{0}{255}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class FloatToUint8Range(nn.Module):\n    def forward(self, input):\n        return input * 255.0\n\n\nclass Uint8ToFloatRange(nn.Module):\n    def forward(self, input):\n        return input / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can put all pieces together.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            Conv(3, 32, kernel_size=9),\n            Conv(32, 64, kernel_size=3, stride=2),\n            Conv(64, 128, kernel_size=3, stride=2),\n            Residual(128),\n            Residual(128),\n            Residual(128),\n            Residual(128),\n            Residual(128),\n        )\n        self.decoder = nn.Sequential(\n            Conv(128, 64, kernel_size=3, stride=2, upsample=True),\n            Conv(64, 32, kernel_size=3, stride=2, upsample=True),\n            Conv(32, 3, kernel_size=9, norm=False, activation=False),\n        )\n\n        self.preprocessor = FloatToUint8Range()\n        self.postprocessor = Uint8ToFloatRange()\n\n    def forward(self, input):\n        input = self.preprocessor(input)\n        output = self.decoder(self.encoder(input))\n        return self.postprocessor(output)\n\n\ntransformer = Transformer().to(device)\nprint(transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perceptual loss\n\nAlthough model optimization is a different paradigm, the perceptual loss is the same\nas for image optimization.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p></p></div>\n\n In some implementations, such as the PyTorch example and :cite:`JAL2016`, one can\n observe that the :func:`~pystiche.gram_matrix`, used as style representation, is not\n only normalized by the height and width of the feature map, but also by the number\n of channels. If used togehter with a :func:`~torch.nn.functional.mse_loss`, the\n normalization is performed twice. While this is unintended, it affects the training.\n In order to keep the other hyper parameters on par with the PyTorch example, we also\n adopt this change here.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "multi_layer_encoder = enc.vgg16_multi_layer_encoder()\n\ncontent_layer = \"relu2_2\"\ncontent_encoder = multi_layer_encoder.extract_encoder(content_layer)\ncontent_weight = 1e5\ncontent_loss = ops.FeatureReconstructionOperator(\n    content_encoder, score_weight=content_weight\n)\n\n\nclass GramOperator(ops.GramOperator):\n    def enc_to_repr(self, enc: torch.Tensor) -> torch.Tensor:\n        repr = super().enc_to_repr(enc)\n        num_channels = repr.size()[1]\n        return repr / num_channels\n\n\nstyle_layers = (\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\")\nstyle_weight = 1e10\nstyle_loss = ops.MultiLayerEncodingOperator(\n    multi_layer_encoder,\n    style_layers,\n    lambda encoder, layer_weight: GramOperator(encoder, score_weight=layer_weight),\n    layer_weights=\"sum\",\n    score_weight=style_weight,\n)\n\ncriterion = loss.PerceptualLoss(content_loss, style_loss)\ncriterion = criterion.to(device)\nprint(criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n\nIn a first step we load the style image that will be used to train the\n``transformer``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "images = demo.images()\nsize = 500\n\nstyle_image = images[\"paint\"].read(size=size, device=device)\nshow_image(style_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training of the ``transformer`` is performed similar to other models in PyTorch.\nIn every optimization step a batch of content images is drawn from a dataset, which\nserve as input for the transformer as well as ``content_image`` for the\n``criterion``. While the ``style_image`` only has to be set once, the\n``content_image`` has to be reset in every iteration step.\n\nWhile this can be done with a boilerplate optimization loop, ``pystiche`` provides\n:func:`~pystiche.optim.multi_epoch_model_optimization` that handles the above for you.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>If the ``criterion`` is a :class:`~pystiche.loss.PerceptualLoss`, as is the case\n  here, the update of the ``content_image`` is performed automatically. If that is\n  not the case or you need more complex update behavior, you need to specify a\n  ``criterion_update_fn``.</p></div>\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>If you do not specify an ``optimizer``, the\n  :func:`~pystiche.optim.default_model_optimizer`, i.e.\n  :class:`~torch.optim.Adam` is used.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train(\n    transformer, dataset, batch_size=4, epochs=2,\n):\n    if dataset is None:\n        raise RuntimeError(\n            \"You forgot to define a dataset. For example, \"\n            \"you can use any image dataset from torchvision.datasets.\"\n        )\n\n    from torch.utils.data import DataLoader\n\n    image_loader = DataLoader(dataset, batch_size=batch_size)\n\n    criterion.set_style_image(style_image)\n\n    return optim.multi_epoch_model_optimization(\n        image_loader,\n        transformer.train(),\n        criterion,\n        epochs=epochs,\n        logger=demo.logger(),\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Depending on the dataset and your setup the training can take a couple of hours. To\navoid this, we provide transformer weights that were trained with the scheme above.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>If you want to perform the training yourself, set\n  ``use_pretrained_transformer=False``. If you do, you also need to replace\n  ``dataset = None`` below with the dataset you want to train on.</p></div>\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The weights of the provided transformer were trained with the\n  `2014 training images <http://images.cocodataset.org/zips/train2014.zip>`_ of the\n  `COCO dataset <https://cocodataset.org/>`_. The training was performed for\n  ``num_epochs=2`` and ``batch_size=4``. Each image was center-cropped to\n  ``256 x 256`` pixels.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "use_pretrained_transformer = True\ncheckpoint = \"example_transformer.pth\"\n\nif use_pretrained_transformer:\n    if path.exists(checkpoint):\n        state_dict = torch.load(checkpoint)\n    else:\n        # Unfortunately, torch.hub.load_state_dict_from_url has no option to disable\n        # printing the downloading process. Since this would clutter the output, we\n        # suppress it completely.\n        @contextlib.contextmanager\n        def suppress_output():\n            with open(os.devnull, \"w\") as devnull:\n                with contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(\n                    devnull\n                ):\n                    yield\n\n        url = \"https://download.pystiche.org/models/example_transformer.pth\"\n\n        with suppress_output():\n            state_dict = hub.load_state_dict_from_url(url)\n\n    transformer.load_state_dict(state_dict)\nelse:\n    dataset = None\n    transformer = train(transformer, dataset)\n\n    state_dict = OrderedDict(\n        [\n            (name, parameter.detach().cpu())\n            for name, parameter in transformer.state_dict().items()\n        ]\n    )\n    torch.save(state_dict, checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Style Transfer\n\nIn order to perform the NST, we load an image we want to stylize.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_image = images[\"bird1\"].read(size=size, device=device)\nshow_image(input_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the transformer is trained we can now perform an NST with a single forward pass.\nTo do this, the ``transformer`` is simply called with the ``input_image``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transformer.eval()\n\nstart = time.time()\n\nwith torch.no_grad():\n    output_image = transformer(input_image)\n\nstop = time.time()\n\nshow_image(output_image, title=\"Output image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compared to NST via image optimization, the stylization is performed multiple orders\nof magnitudes faster. Given capable hardware, NST via model optimization enables\nreal-time stylization for example of a video feed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"The stylization took {(stop - start) * 1e3:.0f} milliseconds.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}