{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Neural Style Transfer without ``pystiche``\n\nThis example showcases how a basic Neural Style Transfer (NST), i.e. image-based\noptimization, could be performed without ``pystiche``.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This is an *example how to implement an NST* and **not** a\n    *tutorial on how NST works*. As such, it will not explain why a specific choice was\n    made or how a component works. If you have never worked with NST before, we\n    **strongly** suggest you to read the `gist` first.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nWe start this example by importing everything we need and setting the device we will\nbe working on. :mod:`torch` and :mod:`torchvision` will be used for the actual NST.\nFurthermore, we use :mod:`PIL.Image` for the file input, and :mod:`matplotlib.pyplot`\nto show the images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import itertools\nfrom collections import OrderedDict\nfrom urllib.request import Request, urlopen\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport torch\nimport torchvision\nfrom torch import nn, optim\nfrom torch.nn.functional import mse_loss\nfrom torchvision import transforms\nfrom torchvision.models import vgg19\nfrom torchvision.transforms.functional import resize\n\nprint(f\"I'm working with torch=={torch.__version__}\")\nprint(f\"I'm working with torchvision=={torchvision.__version__}\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"I'm working with {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The core component of different NSTs is the perceptual loss, which is used as\noptimization criterion. The perceptual loss is usually, and also for this example,\ncalculated on features maps also called encodings. These encodings are generated from\ndifferent layers of a Convolutional Neural Net (CNN) also called encoder.\n\nA common implementation strategy for the perceptual loss is to *weave in* transparent\nloss layers into the encoder. These loss layers are called transparent since from an\noutside view they simply pass the input through without alteration. Internally\nthough, they calculate the loss with the encodings of the previous layer and store\nthem in themselves. After the forward pass is completed the stored losses are\naggregated and propagated backwards to the image. While this is simple to implement,\nthis practice has two downsides:\n\n1. The calculated score is part of the current state but has to be stored inside the\n   layer. This is generally not recommended.\n2. While the encoder is a part of the perceptual loss, it itself does not generate\n   it. One should be able to use the same encoder with a different perceptual loss\n   without modification.\n\nThus, this example (and ``pystiche``) follows a different approach and separates the\nencoder and the perceptual loss into individual entities.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-layer Encoder\n\nIn a first step we define a ``MultiLayerEncoder`` that should have the following\nproperties:\n\n1. Given an image and a set of layers, the ``MultiLayerEncoder`` should return the\n   encodings of every given layer.\n2. Since the encodings have to be generated in every optimization step they should be\n   calculated in a single forward pass to keep the processing costs low.\n3. To reduce the static memory requirement, the ``MultiLayerEncoder`` should be\n   ``trim`` mable in order to remove unused layers.\n\nWe achieve the main functionality by subclassing :class:`torch.nn.Sequential` and\ndefine a custom ``forward`` method, i.e. different behavior if called. Besides the\nimage it also takes an iterable ``layer_cfgs`` containing multiple sequences of\n``layers``. In the method body we first find the ``deepest_layer`` that was\nrequested. Subsequently, we calculate and store all encodings of the ``image`` up to\nthat layer. Finally we can return all requested encodings without processing the same\nlayer twice.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MultiLayerEncoder(nn.Sequential):\n    def forward(self, image, *layer_cfgs):\n        storage = {}\n        deepest_layer = self._find_deepest_layer(*layer_cfgs)\n        for layer, module in self.named_children():\n            image = storage[layer] = module(image)\n            if layer == deepest_layer:\n                break\n\n        return [[storage[layer] for layer in layers] for layers in layer_cfgs]\n\n    def children_names(self):\n        for name, module in self.named_children():\n            yield name\n\n    def _find_deepest_layer(self, *layer_cfgs):\n        # find all unique requested layers\n        req_layers = set(itertools.chain(*layer_cfgs))\n        try:\n            # find the deepest requested layer by indexing the layers within\n            # the multi layer encoder\n            children_names = list(self.children_names())\n            return sorted(req_layers, key=children_names.index)[-1]\n        except ValueError as error:\n            layer = str(error).split()[0]\n        raise ValueError(f\"Layer {layer} is not part of the multi-layer encoder.\")\n\n    def trim(self, *layer_cfgs):\n        deepest_layer = self._find_deepest_layer(*layer_cfgs)\n        children_names = list(self.children_names())\n        del self[children_names.index(deepest_layer) + 1 :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pretrained models the ``MultiLayerEncoder`` is based on are usually trained on\npreprocessed images. In PyTorch all models expect images are\n`normalized <https://pytorch.org/docs/stable/torchvision/models.html>`_ by a\nper-channel ``mean = (0.485, 0.456, 0.406)`` and standard deviation\n(``std = (0.229, 0.224, 0.225)``). To include this into a, ``MultiLayerEncoder``, we\nimplement this as :class:`torch.nn.Module` .\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Normalize(nn.Module):\n    def __init__(self, mean, std):\n        super().__init__()\n        self.register_buffer(\"mean\", torch.tensor(mean).view(1, -1, 1, 1))\n        self.register_buffer(\"std\", torch.tensor(std).view(1, -1, 1, 1))\n\n    def forward(self, image):\n        return (image - self.mean) / self.std\n\n\nclass TorchNormalize(Normalize):\n    def __init__(self):\n        super().__init__((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a last step we need to specify the structure of the ``MultiLayerEncoder``. In this\nexample we use a ``VGGMultiLayerEncoder`` based on the ``VGG19`` CNN introduced by\nSimonyan and Zisserman :cite:`SZ2014`.\n\nWe only include the feature extraction stage (``vgg_net.features``), i.e. the\nconvolutional stage, since the classifier stage (``vgg_net.classifier``) only accepts\nfeature maps of a single size.\n\nFor our convenience we rename the layers in the same scheme the authors used instead\nof keeping the consecutive index of a default :class:`torch.nn.Sequential`. The first\nlayer however is the ``TorchNormalize``  as defined above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class VGGMultiLayerEncoder(MultiLayerEncoder):\n    def __init__(self, vgg_net):\n        modules = OrderedDict(((\"preprocessing\", TorchNormalize()),))\n\n        block = depth = 1\n        for module in vgg_net.features.children():\n            if isinstance(module, nn.Conv2d):\n                layer = f\"conv{block}_{depth}\"\n            elif isinstance(module, nn.BatchNorm2d):\n                layer = f\"bn{block}_{depth}\"\n            elif isinstance(module, nn.ReLU):\n                # without inplace=False the encodings of the previous layer would no\n                # longer be accessible after the ReLU layer is executed\n                module = nn.ReLU(inplace=False)\n                layer = f\"relu{block}_{depth}\"\n                # each ReLU layer increases the depth of the current block by one\n                depth += 1\n            elif isinstance(module, nn.MaxPool2d):\n                layer = f\"pool{block}\"\n                # each max pooling layer marks the end of the current block\n                block += 1\n                depth = 1\n            else:\n                msg = f\"Type {type(module)} is not part of the VGG architecture.\"\n                raise RuntimeError(msg)\n\n            modules[layer] = module\n\n        super().__init__(modules)\n\n\ndef vgg19_multi_layer_encoder():\n    return VGGMultiLayerEncoder(vgg19(pretrained=True))\n\n\nmulti_layer_encoder = vgg19_multi_layer_encoder().to(device)\nprint(multi_layer_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perceptual Loss\n\nIn order to calculate the perceptual loss, i.e. the optimization criterion, we define\na ``MultiLayerLoss`` to have a convenient interface. This will be subclassed later by\nthe ``ContentLoss`` and ``StyleLoss``.\n\nIf called with a sequence of ``\u00ecnput_encs`` the ``MultiLayerLoss`` should calculate\nlayerwise scores together with the corresponding ``target_encs``. For that a\n``MultiLayerLoss`` needs the ability to store the ``target_encs`` so that they can be\nreused for every call. The individual layer scores should be averaged by the number\nof encodings and finally weighted by a ``score_weight``.\n\nTo achieve this we subclass :class:`torch.nn.Module` . The ``target_encs`` are stored\nas buffers, since they are not trainable parameters. The actual functionality has to\nbe defined in ``calculate_score`` by a subclass.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def mean(sized):\n    return sum(sized) / len(sized)\n\n\nclass MultiLayerLoss(nn.Module):\n    def __init__(self, score_weight=1e0):\n        super().__init__()\n        self.score_weight = score_weight\n        self._numel_target_encs = 0\n\n    def _target_enc_name(self, idx):\n        return f\"_target_encs_{idx}\"\n\n    def set_target_encs(self, target_encs):\n        self._numel_target_encs = len(target_encs)\n        for idx, enc in enumerate(target_encs):\n            self.register_buffer(self._target_enc_name(idx), enc.detach())\n\n    @property\n    def target_encs(self):\n        return tuple(\n            getattr(self, self._target_enc_name(idx))\n            for idx in range(self._numel_target_encs)\n        )\n\n    def forward(self, input_encs):\n        if len(input_encs) != self._numel_target_encs:\n            msg = (\n                f\"The number of given input encodings and stored target encodings \"\n                f\"does not match: {len(input_encs)} != {self._numel_target_encs}\"\n            )\n            raise RuntimeError(msg)\n\n        layer_losses = [\n            self.calculate_score(input, target)\n            for input, target in zip(input_encs, self.target_encs)\n        ]\n        return mean(layer_losses) * self.score_weight\n\n    def calculate_score(self, input, target):\n        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example we use the ``feature_reconstruction_loss`` introduced by Mahendran\nand Vedaldi :cite:`MV2015` as ``ContentLoss`` as well as the ``gram_loss`` introduced\nby Gatys, Ecker, and Bethge :cite:`GEB2016` as ``StyleLoss``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def feature_reconstruction_loss(input, target):\n    return mse_loss(input, target)\n\n\nclass ContentLoss(MultiLayerLoss):\n    def calculate_score(self, input, target):\n        return feature_reconstruction_loss(input, target)\n\n\ndef channelwise_gram_matrix(x, normalize=True):\n    x = torch.flatten(x, 2)\n    G = torch.bmm(x, x.transpose(1, 2))\n    if normalize:\n        return G / x.size()[-1]\n    else:\n        return G\n\n\ndef gram_loss(input, target):\n    return mse_loss(channelwise_gram_matrix(input), channelwise_gram_matrix(target))\n\n\nclass StyleLoss(MultiLayerLoss):\n    def calculate_score(self, input, target):\n        return gram_loss(input, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Images\n\nBefore we can load the content and style image, we need to define some basic I/O\nutilities.\n\nAt import a fake batch dimension is added to the images to be able to pass it through\nthe ``MultiLayerEncoder`` without further modification. This dimension is removed\nagain upon export. Furthermore, all images will be resized to ``size=500`` pixels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import_from_pil = transforms.Compose(\n    (\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x.unsqueeze(0)),\n        transforms.Lambda(lambda x: x.to(device)),\n    )\n)\n\nexport_to_pil = transforms.Compose(\n    (\n        transforms.Lambda(lambda x: x.cpu()),\n        transforms.Lambda(lambda x: x.squeeze(0)),\n        transforms.Lambda(lambda x: x.clamp(0.0, 1.0)),\n        transforms.ToPILImage(),\n    )\n)\n\n\ndef download_image(url, file):\n    with open(file, \"wb\") as fh:\n        # without User-Agent the access is denied\n        request = Request(url, headers={\"User-Agent\": \"pystiche\"})\n        with urlopen(request) as response:\n            fh.write(response.read())\n\n\ndef read_image(file, size=500):\n    image = Image.open(file)\n    image = resize(image, size)\n    return import_from_pil(image)\n\n\ndef show_image(image, title=None):\n    _, ax = plt.subplots()\n    ax.axis(\"off\")\n    if title is not None:\n        ax.set_title(title)\n\n    image = export_to_pil(image)\n    ax.imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the I/O utilities set up, we now download, read, and show the images that will\nbe used in the NST.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The images used in this example are licensed under the permissive\n  `Pixabay License <https://pixabay.com/service/license/>`_ .</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "content_url = \"https://cdn.pixabay.com/photo/2016/01/14/11/26/bird-1139734_960_720.jpg\"\ncontent_file = \"bird1.jpg\"\n\ndownload_image(content_url, content_file)\ncontent_image = read_image(content_file)\nshow_image(content_image, title=\"Content image\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "style_url = (\n    \"https://cdn.pixabay.com/photo/2017/07/03/20/17/abstract-2468874_960_720.jpg\"\n)\nstyle_file = \"paint.jpg\"\n\ndownload_image(style_url, style_file)\nstyle_image = read_image(style_file)\nshow_image(style_image, title=\"Style image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Style Transfer\n\nAt first we chose the ``content_layers`` and ``style_layers`` on which the encodings\nare compared. With them we ``trim`` the ``multi_layer_encoder`` to remove\nunused layers that otherwise occupy memory.\n\nAfterwards we calculate the target content and style encodings. The calculation is\nperformed without a gradient since the gradient of the target encodings is not needed\nfor the optimization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "content_layers = (\"relu4_2\",)\nstyle_layers = (\"relu1_1\", \"relu2_1\", \"relu3_1\", \"relu4_1\", \"relu5_1\")\n\nmulti_layer_encoder.trim(content_layers, style_layers)\n\nwith torch.no_grad():\n    target_content_encs = multi_layer_encoder(content_image, content_layers)[0]\n    target_style_encs = multi_layer_encoder(style_image, style_layers)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next up, we instantiate the ``ContentLoss`` and ``StyleLoss`` with a corresponding\nweight. Afterwards we store the previously calculated target encodings.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "content_weight = 1e0\ncontent_loss = ContentLoss(score_weight=content_weight)\ncontent_loss.set_target_encs(target_content_encs)\n\nstyle_weight = 1e3\nstyle_loss = StyleLoss(score_weight=style_weight)\nstyle_loss.set_target_encs(target_style_encs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start NST from the ``content_image`` since this way it converges quickly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_image = content_image.clone()\nshow_image(input_image, \"Input image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>If you want to start from a white noise image instead use\n\n  .. code-block:: python\n\n    input_image = torch.rand_like(content_image)</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a last preliminary step we create the optimizer that will be performing the NST.\nSince we want to adapt the pixels of the ``input_image`` directly, we pass it as\noptimization parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = optim.LBFGS([input_image.requires_grad_(True)], max_iter=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we run the NST. The loss calculation has to happen inside a ``closure``\nsince the :class:`~torch.optim.LBFGS` optimizer could need to\n`reevaluate it multiple times per optimization step <https://pytorch.org/docs/stable/optim.html#optimizer-step-closure>`_\n. This structure is also valid for all other optimizers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_steps = 500\n\nfor step in range(1, num_steps + 1):\n\n    def closure():\n        optimizer.zero_grad()\n\n        input_encs = multi_layer_encoder(input_image, content_layers, style_layers)\n        input_content_encs, input_style_encs = input_encs\n\n        content_score = content_loss(input_content_encs)\n        style_score = style_loss(input_style_encs)\n\n        perceptual_loss = content_score + style_score\n        perceptual_loss.backward()\n\n        if step % 50 == 0:\n            print(f\"Step {step}\")\n            print(f\"Content loss: {content_score.item():.3e}\")\n            print(f\"Style loss:   {style_score.item():.3e}\")\n            print(\"-----------------------\")\n\n        return perceptual_loss\n\n    optimizer.step(closure)\n\noutput_image = input_image.detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the NST we show the resulting image.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "show_image(output_image, title=\"Output image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nAs hopefully has become clear, an NST requires even in its simplest form quite a lot\nof utilities and boilerplate code. This makes it hard to maintain and keep bug free\nas it is easy to lose track of everything.\n\nJudging by the lines of code one could (falsely) conclude that the actual NST is just\nan appendix. If you feel the same you can stop worrying now: in\n`sphx_glr_galleries_examples_beginner_example_nst_with_pystiche.py` we showcase\nhow to achieve the same result with ``pystiche``.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}